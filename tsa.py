# -*- coding: utf-8 -*-
"""TSA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GODLZRzBBw_gvs8F8XNDFQvWxe0riVH6
"""

#installing kaggle library
! pip install kaggle

#configuring the path of Kaggle.json file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Importing Twitter Sentiment dataset"""

#API to fetch the dataset from Kaggle
!kaggle datasets download -d kazanova/sentiment140

# extracting the compressed dataset

from zipfile import ZipFile
dataset = '/content/sentiment140.zip'

with ZipFile(dataset,'r') as zip:  #reading this file as zip
  zip.extractall()                #extracting all the files
  print('The dataset is extracted')

"""Importing the dependencies"""

import numpy as np #basic python library
import pandas as pd #used to create dataframe. we will load csv file in pandas dataframe
import re #regular extraction. pattern matching, search through datasets, etc
from nltk.corpus import stopwords #natural language tool kit, corpus module
from nltk.stem.porter import PorterStemmer #stemming- used to reduce a word to root words
from sklearn.feature_extraction.text import TfidfVectorizer #convert textaul data to numerical
from sklearn.model_selection import train_test_split #split orig data into training and testing data
from sklearn.linear_model import LogisticRegression #used for training model
from sklearn.metrics import accuracy_score #used to evaluate performance our model

import nltk
nltk.download('stopwords')

#printing stopwords in English
print(stopwords.words('english'))

#they don't add sentimental value to the sentence hence not required for processing
#we will remove these from our tweets, cutshot the complex data a bit

"""Data Processing"""

#loading the data from csv file to pandas dataframe
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1')

# checking the number of rows and columns
twitter_data.shape

# print first 5 rows of the dataframe
twitter_data.head()

#naming the columns and reading the dataset again

 column_names = ['target', 'id', 'date', 'flag', 'user', 'text']
 twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', names = column_names, encoding = 'ISO-8859-1')

# checking the number of rows and columns
twitter_data.shape

# print first 5 rows of the dataframe
#now the table will display with column names
twitter_data.head()

# counting the number of missing values in the dataset
twitter_data.isnull().sum()

# checking the dirstribution of target column
twitter_data['target'].value_counts() #no of values for 0 and 4
#no of positive and negative tweets

"""Convert the target "4" to "1"
"""

# so labels are 0: negative and 1:positive
twitter_data.replace({'target':{4:1}}, inplace=True)

# rechecking the dirstribution of target column
twitter_data['target'].value_counts() #no of values for 0 and 1
#no of positive and negative tweets

"""**Stemming**

A process of reducing a word to its Root word.

eg, actor, actress, acting => act
"""

#using porterstemmer for stemming
port_stem = PorterStemmer()

def stemming(content):
  stemmed_content = re.sub('[^a-zA-Z]',' ',content) #replacing everything except alphabets, ^ means remove the non alphabets
  stemmed_content = stemmed_content.lower() #converting to lower case
  stemmed_content = stemmed_content.split() #splitting the content
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content = ' '.join(stemmed_content)

  return stemmed_content

"""we're going to pass the tweets here to process them (ml model can understand 0, 1 but not words but before having it feeding it the text data it needs to be preprocessed)."""

